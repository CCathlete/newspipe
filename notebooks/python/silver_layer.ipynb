{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import TypeAlias\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "try:\n",
    "    root_path: Path = Path(__file__).parents[2]\n",
    "except NameError:\n",
    "    root_path: Path = Path.cwd()\n",
    "\n",
    "if str(root_path) not in sys.path:\n",
    "    sys.path.append(str(root_path))\n",
    "\n",
    "load_dotenv(root_path / \".env\")\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from src.control.dependency_layers import DataPlatformContainer\n",
    "from pyspark.sql import functions as F\n",
    "from returns.result import ResultE, Success, Failure, safe\n",
    "import ipykernel\n",
    "print(ipykernel.get_connection_file())\n",
    "\n",
    "\n",
    "ConfigType: TypeAlias = dict[str, dict[str, str] | str]\n",
    "\n",
    "testval: int = 8\n",
    "\n",
    "\n",
    "def _resolve_and_validate_lakehouse_config(config: dict) -> dict[str, str]:\n",
    "    assert isinstance(config['lakehouse'], dict)\n",
    "    endpoint = config['lakehouse']['endpoint']\n",
    "    access_key = config['lakehouse']['username']\n",
    "    secret_key = config['lakehouse']['password']\n",
    "    bronze_path = config['lakehouse']['bronze_path']\n",
    "    spark_mode = config['spark_mode']\n",
    "    \n",
    "    missing = []\n",
    "    if not endpoint: missing.append(\"lakehouse.endpoint\")\n",
    "    if not access_key: missing.append(\"lakehouse.username\")\n",
    "    if not secret_key: missing.append(\"lakehouse.password\")\n",
    "    if not bronze_path: missing.append(\"lakehouse.bronze_path\")\n",
    "    \n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing keys: {missing}\")\n",
    "        \n",
    "    return {\n",
    "        \"endpoint\": endpoint,\n",
    "        \"access_key\": access_key,\n",
    "        \"secret_key\": secret_key,\n",
    "        \"bronze_path\": bronze_path,\n",
    "        \"spark_mode\": spark_mode\n",
    "    }\n",
    "\n",
    "\n",
    "def _create_spark_session(resolved_lakehouse_cfg_dict) -> SparkSession:\n",
    "    builder: SparkSession.Builder = SparkSession.Builder()\n",
    "    return (\n",
    "            builder\n",
    "            .master(resolved_lakehouse_cfg_dict['spark_mode'])\n",
    "            .appName(\"NewsAnalysis\")\n",
    "            .config(\n",
    "                \"spark.jars.packages\",\n",
    "                \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "                \"org.apache.hadoop:hadoop-common:3.3.4,\"\n",
    "                \"com.amazonaws:aws-java-sdk-bundle:1.12.262,\"\n",
    "                \"org.apache.spark:spark-hadoop-cloud_2.12:3.5.1\"\n",
    "            )\n",
    "            # This tells Spark not to look for the \"Magic\" or \"S3A\" specific\n",
    "            # committers that are failing to find their class.\n",
    "            .config(\"spark.hadoop.fs.s3a.committer.name\", \"directory\")\n",
    "            .config(\"spark.sql.sources.commitProtocolClass\",\n",
    "                    \"org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol\")\n",
    "            # --- MinIO Specifics ---\n",
    "            .config(\"spark.hadoop.fs.s3a.endpoint\", resolved_lakehouse_cfg_dict['endpoint'])\n",
    "            .config(\"spark.hadoop.fs.s3a.access.key\", resolved_lakehouse_cfg_dict['access_key'])\n",
    "            .config(\"spark.hadoop.fs.s3a.secret.key\", resolved_lakehouse_cfg_dict['secret_key'])\n",
    "            .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "            .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "            .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "            # --- S3A Retry/Timeout Configurations ---\n",
    "            .config(\"spark.hadoop.fs.s3a.attempts.maximum\", \"5\")\n",
    "            .config(\"spark.hadoop.fs.s3a.retry.limit\", \"10\")\n",
    "            .config(\"spark.hadoop.fs.s3a.retry.interval\", \"5000\")\n",
    "            .config(\"spark.hadoop.fs.s3a.establish.timeout\", \"5000\")\n",
    "            .config(\"spark.hadoop.fs.s3a.socket.timeout\", \"60000\")\n",
    "            # Memory config.\n",
    "            .config(\"spark.driver.cores\", \"1\")\n",
    "            .config(\"spark.driver.memory\", \"1g\")\n",
    "            .getOrCreate()\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(testval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@safe\n",
    "def initialize_platform() -> tuple[SparkSession, DataPlatformContainer, ConfigType]:\n",
    "    config: ConfigType = {\n",
    "        \"lakehouse\": {\n",
    "            \"bronze_path\": \"s3a://lakehouse/bronze/**\",\n",
    "            \"endpoint\": \"http://localhost:9000\",\n",
    "            \"username\": os.getenv(\"MINIO_ACCESS_KEY\", \"\"),\n",
    "            \"password\": os.getenv(\"MINIO_SECRET_KEY\", \"\"),\n",
    "        },\n",
    "        \"spark_mode\": \"local[*]\",\n",
    "    }\n",
    "    resolved_lakehouse_cfg_dict: dict[str, str] = _resolve_and_validate_lakehouse_config(config)\n",
    "\n",
    "    container: DataPlatformContainer = DataPlatformContainer() # Future use.\n",
    "    container.config.from_dict(config)\n",
    "\n",
    "    spark: SparkSession = _create_spark_session(resolved_lakehouse_cfg_dict)\n",
    "\n",
    "    return spark, container, config\n",
    "\n",
    "setup_result: ResultE[tuple[SparkSession, DataPlatformContainer, ConfigType]] = initialize_platform()\n",
    "\n",
    "spark: SparkSession\n",
    "\n",
    "match setup_result:\n",
    "    case Success((spark, container, config)):\n",
    "        print(\"Spark Session Active\")\n",
    "        lake_config: dict[str, str] | str = config[\"lakehouse\"]\n",
    "    case Failure(err):\n",
    "        print(f\"Setup Failed: {err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@safe\n",
    "def load_bronze(spark: SparkSession, path: str) -> DataFrame:\n",
    "    df: DataFrame = (\n",
    "    spark.read\n",
    "    .option(\"recursiveFileLookup\", \"true\")\n",
    "    .option(\"mode\", \"PERMISSIVE\")\n",
    "    .json(path)\n",
    "    )\n",
    "\n",
    "    return df.withColumn(\n",
    "        \"source\",\n",
    "        F.regexp_extract(F.input_file_name(), r\"bronze/([^/]+)/\", 1)\n",
    "    )\n",
    "\n",
    "\n",
    "@safe\n",
    "def inspect_prefix(spark: SparkSession, path: str) -> DataFrame:\n",
    "    return spark.read.format(\"binaryFile\").load(path)\n",
    "\n",
    "\n",
    "bronze_result: ResultE[DataFrame] = Failure(Exception(\"Initial value for bronze result.\"))\n",
    "\n",
    "match setup_result:\n",
    "    case Success((spark, container, config)):\n",
    "        lakehouse_dict: dict[str, str] | str = config[\"lakehouse\"]\n",
    "        assert isinstance(lakehouse_dict, dict)\n",
    "        bronze_path: str = lakehouse_dict[\"bronze_path\"]\n",
    "\n",
    "        sanity_test: ResultE[DataFrame] = inspect_prefix(spark, bronze_path)\n",
    "        bronze_result: ResultE[DataFrame] = load_bronze(spark, bronze_path)\n",
    "        \n",
    "        match bronze_result:\n",
    "            case Success(df):\n",
    "                print(\"Bronze Data Loaded\")\n",
    "                df.printSchema()\n",
    "                bronze_df: DataFrame = df\n",
    "            case Failure(err):\n",
    "                print(f\"Load Failed: {err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "@safe\n",
    "def inspect_data(df: DataFrame) -> int:\n",
    "    # df.show(5, truncate=False)\n",
    "    df.show(5, truncate=True)\n",
    "    return df.count()\n",
    "\n",
    "match setup_result:\n",
    "    case Success((spark, container, config)):\n",
    "        match (bronze_result):\n",
    "            case Success(bronze_df):\n",
    "                inspection: ResultE[int] = inspect_data(bronze_df)\n",
    "                bronze_df.createOrReplaceTempView(\"bronze_df\")\n",
    "                sorted_df: DataFrame = spark.sql(\"\"\"\n",
    "                    SELECT\n",
    "                        source,\n",
    "                        content,\n",
    "                        CAST(FROM_UNIXTIME(CAST(ingested_at AS BIGINT)) AS TIMESTAMP) AS ingested_timestamp\n",
    "                    FROM bronze_df\n",
    "                    ORDER BY source ASC, ingested_at ASC\n",
    "                    ;\n",
    "                    \"\"\")\n",
    "                sorted_df.createOrReplaceTempView(\"sorted_by_ingested\")\n",
    "                # We turn grouped chunks to list of tuples and sort them by ingested_timestamp.\n",
    "                # We then take only the content from each tuple and concat.\n",
    "                reconstructed_df: DataFrame = spark.sql(\"\"\"\n",
    "                    SELECT\n",
    "                        source,\n",
    "                        concat_ws(\n",
    "                            '',\n",
    "                            transform(\n",
    "                                sort_array(\n",
    "                                    collect_list(struct(ingested_timestamp, content))\n",
    "                                ),\n",
    "                                x -> x.content\n",
    "                            )\n",
    "                        ) AS full_content\n",
    "                    FROM sorted_by_ingested\n",
    "                    GROUP BY source\n",
    "                    ;\n",
    "                \"\"\")\n",
    "                reconstructed_df.createOrReplaceTempView(\"reconstructed_chunks\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                match inspection:\n",
    "                    case Success(count):\n",
    "                        print(f\"Total records in Bronze: {count}\")\n",
    "                    case Failure(err):\n",
    "                        print(f\"Inspection Error: {err}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3",
   "path": "/usr/share/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
